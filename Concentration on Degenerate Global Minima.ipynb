{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Synthetic data: y = Xw + noise\n",
    "# We'll choose N << P to create an overparameterized scenario.\n",
    "N = 50\n",
    "P = 2000\n",
    "X = torch.randn(N, P)\n",
    "true_w = torch.randn(P) * 0.1\n",
    "y = X @ true_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple linear model\n",
    "class OverParamLinear(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(OverParamLinear, self).__init__()\n",
    "        self.w = nn.Parameter(torch.zeros(p))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X @ self.w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = OverParamLinear(P)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# SGLD-like updates:\n",
    "lr = 1e-3\n",
    "T = 1e-4  # \"Temperature\"\n",
    "steps = 20000\n",
    "burn_in = 10000\n",
    "\n",
    "w_samples = []\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for step in range(steps):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "\n",
    "    # Standard SGD step\n",
    "    for p in model.parameters():\n",
    "        # Add Gaussian noise for SGLD\n",
    "        noise = torch.randn_like(p) * np.sqrt(2 * lr * T)\n",
    "        p.data = p.data - lr * p.grad.data + noise\n",
    "\n",
    "    # Collect samples after burn-in\n",
    "    if step > burn_in and step % 10 == 0:\n",
    "        w_samples.append(model.w.data.clone())\n",
    "\n",
    "w_samples = torch.stack(w_samples)  # shape: [num_samples, P]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check flatness: For each sample, perturb and check loss increase\n",
    "def flatness_score(w, eps=0.01, directions=10):\n",
    "    base_loss = criterion(X @ w, y).item()\n",
    "    increases = []\n",
    "    for _ in range(directions):\n",
    "        direction = torch.randn_like(w)\n",
    "        direction = direction / direction.norm()\n",
    "        w_pert = w + eps * direction\n",
    "        pert_loss = criterion(X @ w_pert, y).item()\n",
    "        increases.append(pert_loss - base_loss)\n",
    "    return np.mean(increases)\n",
    "\n",
    "scores = [flatness_score(ws) for ws in w_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(scores, bins=30, color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Average Loss Increase under Perturbation', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "# plt.title('Distribution of Flatness Scores for Sampled Solutions', fontsize=16)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('flatness_distribution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# If most sampled solutions have low loss-increase after perturbation,\n",
    "# it indicates they lie in flat regions. As P is large and no unique\n",
    "# isolated solution exists, we expect the stationary measure to find\n",
    "# these large, flat solution sets."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
