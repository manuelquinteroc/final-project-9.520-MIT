{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Data Loading\n",
    "##############################################\n",
    "\n",
    "# We use MNIST as the dataset. We will also create a subset for faster training.\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Choose a small subset for demonstration\n",
    "subset_indices = np.random.choice(len(mnist_train), 1000, replace=False)\n",
    "mnist_subset = Subset(mnist_train, subset_indices)\n",
    "train_loader = DataLoader(mnist_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create random label dataset:\n",
    "random_labels = torch.randint(low=0, high=10, size=(len(mnist_subset),))\n",
    "random_train_dataset = [(img, random_labels[idx].item()) for idx, (img, lbl) in enumerate(mnist_subset)]\n",
    "random_train_loader = DataLoader(random_train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################\n",
    "# Network Definitions\n",
    "##############################################\n",
    "\n",
    "class OverParamNet(nn.Module):\n",
    "    def __init__(self, hidden_size=5000):  # Large network (overparameterized)\n",
    "        super(OverParamNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class UnderParamNet(nn.Module):\n",
    "    def __init__(self, hidden_size=50):  # Smaller network (underparameterized)\n",
    "        super(UnderParamNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################\n",
    "# Training Function\n",
    "##############################################\n",
    "\n",
    "def train_model(train_loader, model_class, lr=0.1, max_epochs=30):\n",
    "    device = torch.device('cpu')\n",
    "    model = model_class().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = correct / total\n",
    "        # If we perfectly fit the data, we can stop early.\n",
    "        if train_acc == 1.0:\n",
    "            break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################\n",
    "# Measure Flatness\n",
    "##############################################\n",
    "\n",
    "def measure_loss_increase(model_state_dict, model_class, train_loader, epsilons, num_directions=5):\n",
    "    device = torch.device('cpu')\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Compute baseline loss\n",
    "    baseline_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            batch_size = labels.size(0)\n",
    "            baseline_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "    baseline_loss /= total_samples\n",
    "\n",
    "    # Flatten parameters\n",
    "    params = torch.cat([p.detach().view(-1) for p in model.parameters()])\n",
    "    dim = params.numel()\n",
    "\n",
    "    mean_increases = []\n",
    "    std_increases = []\n",
    "\n",
    "    for eps in epsilons:\n",
    "        increases = []\n",
    "        for _ in range(num_directions):\n",
    "            direction = torch.randn(dim, device=device)\n",
    "            direction = direction / torch.norm(direction)\n",
    "            perturbed_params = params + eps * direction\n",
    "\n",
    "            # Load perturbed params back into model\n",
    "            idx = 0\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    size = p.numel()\n",
    "                    new_vals = perturbed_params[idx:idx+size].view(p.size())\n",
    "                    p.copy_(new_vals)\n",
    "                    idx += size\n",
    "\n",
    "            # Compute loss for perturbed model\n",
    "            pert_loss = 0.0\n",
    "            total_samples = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in train_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    batch_size = labels.size(0)\n",
    "                    pert_loss += loss.item() * batch_size\n",
    "                    total_samples += batch_size\n",
    "            pert_loss /= total_samples\n",
    "\n",
    "            increases.append(pert_loss - baseline_loss)\n",
    "\n",
    "        mean_increases.append(np.mean(increases))\n",
    "        std_increases.append(np.std(increases))\n",
    "\n",
    "    return mean_increases, std_increases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################\n",
    "# Experiments\n",
    "##############################################\n",
    "\n",
    "# Train overparameterized and underparameterized networks on natural labels\n",
    "solutions_overparam = []\n",
    "solutions_underparam = []\n",
    "\n",
    "for i in range(3):\n",
    "    sol_over = train_model(train_loader, OverParamNet)\n",
    "    solutions_overparam.append(sol_over.state_dict())\n",
    "\n",
    "    sol_under = train_model(train_loader, UnderParamNet)\n",
    "    solutions_underparam.append(sol_under.state_dict())\n",
    "\n",
    "# Train overparameterized networks on random labels\n",
    "solutions_random = []\n",
    "for i in range(3):\n",
    "    sol_rand = train_model(random_train_loader, OverParamNet)\n",
    "    solutions_random.append(sol_rand.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Measure Flatness and Plot\n",
    "##############################################\n",
    "\n",
    "epsilons = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "mean_inc_overparam = []\n",
    "for sol_sd in solutions_overparam:\n",
    "    m, s = measure_loss_increase(sol_sd, OverParamNet, train_loader, epsilons)\n",
    "    mean_inc_overparam.append(m)\n",
    "\n",
    "mean_inc_underparam = []\n",
    "for sol_sd in solutions_underparam:\n",
    "    m, s = measure_loss_increase(sol_sd, UnderParamNet, train_loader, epsilons)\n",
    "    mean_inc_underparam.append(m)\n",
    "\n",
    "mean_inc_random = []\n",
    "for sol_sd in solutions_random:\n",
    "    m, s = measure_loss_increase(sol_sd, OverParamNet, random_train_loader, epsilons)\n",
    "    mean_inc_random.append(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors\n",
    "color_overparam = 'steelblue'\n",
    "color_underparam = 'darkred'\n",
    "\n",
    "# Plot: Overparam vs Underparam\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(\n",
    "    epsilons,\n",
    "    np.mean(mean_inc_overparam, axis=0),\n",
    "    yerr=np.std(mean_inc_overparam, axis=0),\n",
    "    label='Overparameterized (Natural)',\n",
    "    fmt='o-', color=color_overparam, capsize=5, markersize=8, linewidth=2\n",
    ")\n",
    "plt.errorbar(\n",
    "    epsilons,\n",
    "    np.mean(mean_inc_underparam, axis=0),\n",
    "    yerr=np.std(mean_inc_underparam, axis=0),\n",
    "    label='Underparameterized (Natural)',\n",
    "    fmt='s--', color=color_underparam, capsize=5, markersize=8, linewidth=2\n",
    ")\n",
    "plt.xlabel('Perturbation Magnitude ($\\epsilon$)', fontsize=14)\n",
    "plt.ylabel('Loss Increase', fontsize=14)\n",
    "# plt.title('Flatness Comparison: Overparam vs Underparam', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('flatness_over_vs_under.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot: Natural vs Random (both Overparam)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(\n",
    "    epsilons,\n",
    "    np.mean(mean_inc_overparam, axis=0),\n",
    "    yerr=np.std(mean_inc_overparam, axis=0),\n",
    "    label='Overparam (Natural)',\n",
    "    fmt='o-', color=color_overparam, capsize=5, markersize=8, linewidth=2\n",
    ")\n",
    "plt.errorbar(\n",
    "    epsilons,\n",
    "    np.mean(mean_inc_random, axis=0),\n",
    "    yerr=np.std(mean_inc_random, axis=0),\n",
    "    label='Overparam (Random Labels)',\n",
    "    fmt='s--', color=color_underparam, capsize=5, markersize=8, linewidth=2\n",
    ")\n",
    "plt.xlabel('Perturbation Magnitude ($\\epsilon$)', fontsize=14)\n",
    "plt.ylabel('Loss Increase', fontsize=14)\n",
    "#plt.title('Flatness Comparison: Natural vs Random Labels (Overparam)', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('flatness_natural_vs_random.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
